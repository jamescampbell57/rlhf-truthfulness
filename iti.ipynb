{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: Get ITI working on 1) BERT models and 2) LLaMa Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from jaxtyping import Float, Int\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML\n",
    "\n",
    "import pysvelte\n",
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "# TODO\n",
    "MODEL_PATH='llama_model/hf_llama_model'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH)\n",
    "hf_model = LlamaForCausalLM.from_pretrained(MODEL_PATH, low_cpu_mem_usage = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\"llama-7b\", hf_model=hf_model, device=device, fold_ln=False, center_writing_weights=False, center_unembed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor, renderer=None, **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, **kwargs):\n",
    "    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    ")\n",
    "\n",
    "model.tokenizer = tokenizer\n",
    "model.tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "\n",
    "model.generate(\"The capital of Germany is\", max_new_tokens=20, temperature=0)\n",
    "\n",
    "outputs, cache = model.run_with_cache(\"The capital of Australia is Canberra, the country’s largest inland city. It is located in the Australian Capital Territory (ACT), which is a federal territory. The city is located on the banks of the Molonglo River, 280 km south-west of Sydney.\\nThe city was founded in 1913 as a compromise between the six Australian colonies. The city was built to be the capital of the country, and it was named after the Aboriginal word for “meeting place”.\\nThe city is the seat of the Australian Parliament, and the Parliament House is the most important building in the city. The Parliament House is a modern building, which was built in the 1980s. It is a circular building, which is surrounded by a lake.\\nThe city is also the seat of the Australian Government. The city is the home of the High Court of Australia, the Supreme Court of the Australian Capital Territory, and the Australian Def\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einsum\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Goal: define a function to take in hyperparameters Alpha and K, model probe values, and model activations to calculate the new activations\n",
    "# Need to calculate alpha * sigma * theta\n",
    "# theta is the truthful direction given by probe parameter\n",
    "# sigma: standard deviation of head activation along truthful_dir (theta, either mass mean shift or probe weight direction)\n",
    "# adapted from get_interventions_dict in Kenneth Li github\n",
    "\n",
    "def get_act_std(head_activation, truthful_dir): # calculates standard deviations for one head\n",
    "    \"\"\"\n",
    "    head_activation: (batch, d_model,)\n",
    "    # truthful_dir: (d_model, )\n",
    "    \"\"\"\n",
    "    truthful_dir /= torch.norm(truthful_dir, dim=-1, keepdim=True)\n",
    "    proj_act = einops.einsum(head_activation, truthful_dir , \"b d_m, d_m -> b d_m\")\n",
    "    return torch.std(proj_act, dim=0) # (d_m)\n",
    "\n",
    "# truthful direction is difference in mean \n",
    "# returns (*, d_model)\n",
    "def get_mass_mean_dir(all_activations, truth_indices): # \n",
    "    \"\"\"\n",
    "    all_activations: (batch, *, d_model)\n",
    "    truth_indices: (batch, )\n",
    "    \"\"\"\n",
    "    # print(f\"shape of activations is {all_activations.shape}\")\n",
    "    # print(f\"shape of truth_indices is {truth_indices.shape}\")\n",
    "    true_mass_mean = torch.mean(all_activations[truth_indices == 1], dim=0) #(*, d_model)\n",
    "    false_mass_mean = torch.mean(all_activations[truth_indices == 0], dim=0)\n",
    "    # (* d_model)\n",
    "\n",
    "    return (true_mass_mean - false_mass_mean) / (true_mass_mean - false_mass_mean).norm()\n",
    "\n",
    "# truthful direction is probe weight\n",
    "# def get_probe_dirs(probe_list):\n",
    "#     # probe is a list (n_heads len) of LogisticRegression objects\n",
    "#     coefs = []\n",
    "#     for probe in probe_list:\n",
    "#         coefs.append(probe.coef_)\n",
    "        \n",
    "#     return torch.tensor(coefs, dtype=torch.float32, device=device)\n",
    "\n",
    "def get_probe_dir(probe):\n",
    "    probe_weights = torch.tensor(probe.coef_, dtype=torch.float32, device=device).squeeze()\n",
    "    return probe_weights / probe_weights.norm(dim=-1)\n",
    "\n",
    "\n",
    "# calculate the ITI addition (sigma * theta) for one head\n",
    "# uses either MMD or probe\n",
    "def calc_truth_proj(activation, use_MMD=False, use_probe=False, truth_indices=None, probe=None):\n",
    "    '''\n",
    "    activation is (batch, d_m)\n",
    "    '''\n",
    "    if use_MMD: # use mass mean direction -- average difference between true and false classified prompts (only one head)\n",
    "        assert truth_indices is not None\n",
    "        truthful_dir = get_mass_mean_dir(activation, truth_indices)\n",
    "    else: # probe -- just the coefficients of the probe\n",
    "        assert use_probe\n",
    "        assert probe is not None\n",
    "        truthful_dir = get_probe_dir(probe)\n",
    "\n",
    "    # print(f\"Old truthful dir direc is {truthful_dir.shape}\")\n",
    "    truthful_dir /= truthful_dir.norm(dim=-1)\n",
    "    # print(f\"New truthful dir direc is {truthful_dir.shape}\")\n",
    "    act_std = get_act_std(activation, truthful_dir)\n",
    "    \n",
    "    return einops.einsum(act_std, truthful_dir, \"d_m, d_m -> d_m\")\n",
    "\n",
    "def patch_activation_hook_fn(activations, hook: HookPoint, head, old_activations, use_MMD=True, use_probe=False, truth_indices=None, probe=None):\n",
    "    \"\"\"\n",
    "    activations: (batch, n_heads, d_model)\n",
    "    hook: HookPoint\n",
    "    term_to_add: (*, d_model)\n",
    "\n",
    "    A hook that is meant to act on the \"z\" (output) of a given head, and add the \"term_to_add\" on top of it. Only meant to work a certain head. Will broadcast.\n",
    "    \"\"\"\n",
    "    # print(f\"in hook fn, old act shape is {old_activations.shape}\")\n",
    "    term_to_add = calc_truth_proj(old_activations[:,head], use_MMD, use_probe, truth_indices, probe)\n",
    "    # print(f\"v shape is {term_to_add.shape}\")\n",
    "    # print(f\"activations shape is {activations.shape}\")\n",
    "    activations[:,-1,head] += term_to_add\n",
    "\n",
    "# Calculates new_activations for topk and adds temporary hooks\n",
    "def patch_top_activations(model, probe_accuracies, old_activations, topk=20, alpha=20, use_MMD=False, use_probe=False, truth_indices=None, probes=None):\n",
    "    '''\n",
    "    probe_accuracies: (n_layers, n_heads)\n",
    "    old_activations: (batch, n_layers, n_heads, d_model)\n",
    "\n",
    "    if use_probe is True, probes should be list in shape (n_layers, n_heads) filled with probes\n",
    "\n",
    "    Goes into every single activation, and then tells it to add the ITI\n",
    "    '''\n",
    "\n",
    "    # print(f\"old activations shape is {old_activations.shape}\")\n",
    "\n",
    "    top_head_indices = torch.topk(einops.rearrange(probe_accuracies, \"n_l n_h -> (n_l n_h)\"), k=topk).indices # take top k indices\n",
    "    top_head_bools = torch.zeros(size=(probe_accuracies.shape[0] * probe_accuracies.shape[1],)) # set all the ones that aren't top to 0\n",
    "\n",
    "    top_head_bools[top_head_indices] = torch.ones_like(top_head_bools[top_head_indices]) # set all the ones that are top to 1\n",
    "    top_head_bools = einops.rearrange(top_head_bools, \"(n_l n_h) -> n_l n_h\", n_l=model.cfg.n_layers) # rearrange back\n",
    "    \n",
    "    for layer in range(probe_accuracies.shape[0]):\n",
    "        for head in range(probe_accuracies.shape[1]):\n",
    "            if top_head_bools[layer, head] == 1:\n",
    "\n",
    "                if use_probe:\n",
    "                    patch_activation_with_head = partial(patch_activation_hook_fn, head = head, old_activations = old_activations[:, layer], use_MMD=False, use_probe=use_probe, truth_indices=None, probe=probes[layer][head])\n",
    "                else:\n",
    "                    patch_activation_with_head = partial(patch_activation_hook_fn, head = head, old_activations = old_activations[:, layer], use_MMD=use_MMD, use_probe=False, truth_indices=truth_indices, probe=None)\n",
    "                model.add_hook(utils.get_act_name(\"result\", layer), patch_activation_with_head)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
