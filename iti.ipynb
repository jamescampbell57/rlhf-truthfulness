{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: Get ITI working on 1) BERT models and 2) LLaMa Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very fast installation of LLaMA weights via decapoda-research/llama-7b-hf\n",
    "!pip install sentencepiece\n",
    "!apt install git-lfs\n",
    "!git lfs install\n",
    "!git clone https://huggingface.co/decapoda-research/llama-7b-hf\n",
    "!git clone https://huggingface.co/decapoda-research/llama-13b-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LLaMATokenizer' from 'transformers' (/usr/local/lib/python3.8/dist-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m HTML\n\u001b[1;32m     24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m LlamaForCausalLM, LLaMATokenizer, LlamaTokenizer, AutoTokenizer\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maccelerate\u001b[39;00m \u001b[39mimport\u001b[39;00m init_empty_weights, load_checkpoint_and_dispatch\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhuggingface_hub\u001b[39;00m \u001b[39mimport\u001b[39;00m snapshot_download\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LLaMATokenizer' from 'transformers' (/usr/local/lib/python3.8/dist-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from jaxtyping import Float, Int\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, LlamaForCausalLM\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, AutoTokenizer\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "from huggingface_hub import snapshot_download\n",
    "import sentencepiece\n",
    "\n",
    "# import pysvelte\n",
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "def imshow(tensor, renderer=None, **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, **kwargs):\n",
    "    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from local\n",
    "\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "# TODO\n",
    "MODEL_PATH='llama_model/hf_llama_model'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH)\n",
    "hf_model = LlamaForCausalLM.from_pretrained(MODEL_PATH, low_cpu_mem_usage = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3891c2f3e0542158dafc653dac8c62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 42 files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/root/.cache/huggingface/hub/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from huggingface\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from accelerate import load_checkpoint_and_dispatch, init_empty_weights\n",
    "\n",
    "checkpoint_location = snapshot_download(\"decapoda-research/llama-7b-hf\")\n",
    "checkpoint_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tokenizer class LLaMATokenizer does not exist or is not currently imported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[0;32m----> 3\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mdecapoda-research/llama-7b-hf\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mdecapoda-research/llama-7b-hf\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/auto/tokenization_auto.py:688\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m         tokenizer_class \u001b[39m=\u001b[39m tokenizer_class_from_name(tokenizer_class_candidate)\n\u001b[1;32m    687\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 688\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    689\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTokenizer class \u001b[39m\u001b[39m{\u001b[39;00mtokenizer_class_candidate\u001b[39m}\u001b[39;00m\u001b[39m does not exist or is not currently imported.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    690\u001b[0m         )\n\u001b[1;32m    691\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    693\u001b[0m \u001b[39m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    694\u001b[0m \u001b[39m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Tokenizer class LLaMATokenizer does not exist or is not currently imported."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7cba69941c4ac9a903887572a52050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "with init_empty_weights():\n",
    "    model = LlamaForCausalLM.from_pretrained(checkpoint_location)\n",
    "model = load_checkpoint_and_dispatch(\n",
    "    model,\n",
    "    checkpoint_location,\n",
    "    device_map=\"auto\",\n",
    "    no_split_module_classes=[\"LlamaDecoderLayer\"],\n",
    ")\n",
    "\n",
    "tok = LlamaTokenizer.from_pretrained(checkpoint_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"llama.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LLaMA tokenizer not loaded. Please load manually.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 22.19 GiB total capacity; 21.39 GiB already allocated; 122.50 MiB free; 21.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m HookedTransformer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mllama-7b\u001b[39;49m\u001b[39m\"\u001b[39;49m, hf_model\u001b[39m=\u001b[39;49mmodel, device\u001b[39m=\u001b[39;49mdevice, fold_ln\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, center_writing_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, center_unembed\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformer_lens/HookedTransformer.py:758\u001b[0m, in \u001b[0;36mHookedTransformer.from_pretrained\u001b[0;34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, move_state_dict_to_device, **model_kwargs)\u001b[0m\n\u001b[1;32m    753\u001b[0m state_dict \u001b[39m=\u001b[39m loading\u001b[39m.\u001b[39mget_pretrained_state_dict(\n\u001b[1;32m    754\u001b[0m     official_model_name, cfg, hf_model\n\u001b[1;32m    755\u001b[0m )\n\u001b[1;32m    757\u001b[0m \u001b[39m# Create the HookedTransformer object\u001b[39;00m\n\u001b[0;32m--> 758\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(cfg, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m    760\u001b[0m model\u001b[39m.\u001b[39mload_and_process_state_dict(\n\u001b[1;32m    761\u001b[0m     state_dict,\n\u001b[1;32m    762\u001b[0m     fold_ln\u001b[39m=\u001b[39mfold_ln,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    766\u001b[0m     move_state_dict_to_device\u001b[39m=\u001b[39mmove_state_dict_to_device,\n\u001b[1;32m    767\u001b[0m )\n\u001b[1;32m    769\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoaded pretrained model \u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m into HookedTransformer\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformer_lens/HookedTransformer.py:161\u001b[0m, in \u001b[0;36mHookedTransformer.__init__\u001b[0;34m(self, cfg, tokenizer, move_to_device)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_weights()\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m move_to_device:\n\u001b[1;32m    158\u001b[0m     \u001b[39m# We load the devices in a pipeline manner - the first device gets the embed and pos_embed layers and the first n_layers // n_devices blocks,\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[39m# the second gets the next n_layers // n_devices blocks ... the last gets the last n_layers // n_devices blocks, the final\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[39m# normalization layer (if it exists) and the unembed layer\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m     HookedTransformer\u001b[39m.\u001b[39;49mmove_model_modules_to_device(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    163\u001b[0m \u001b[39m# Helper variable to store a small (10K-20K) dataset of training data. Empty by default, can be loaded with load_sample_training_dataset\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformer_lens/HookedTransformer.py:679\u001b[0m, in \u001b[0;36mHookedTransformer.move_model_modules_to_device\u001b[0;34m(cls, model)\u001b[0m\n\u001b[1;32m    675\u001b[0m model\u001b[39m.\u001b[39munembed\u001b[39m.\u001b[39mto(\n\u001b[1;32m    676\u001b[0m     devices\u001b[39m.\u001b[39mget_device_for_block_index(model\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mn_layers \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, model\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m \u001b[39mfor\u001b[39;00m i, block \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(model\u001b[39m.\u001b[39mblocks):\n\u001b[0;32m--> 679\u001b[0m     block\u001b[39m.\u001b[39;49mto(devices\u001b[39m.\u001b[39;49mget_device_for_block_index(i, model\u001b[39m.\u001b[39;49mcfg))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1170\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1167\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1168\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1170\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:822\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    821\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 822\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    824\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    825\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    826\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    827\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    832\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    833\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:845\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    843\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 845\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    846\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    847\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1168\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1166\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1167\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1168\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 22.19 GiB total capacity; 21.39 GiB already allocated; 122.50 MiB free; 21.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"llama-7b\", hf_model=model, device=device, fold_ln=False, center_writing_weights=False, center_unembed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    ")\n",
    "\n",
    "model.tokenizer = tokenizer\n",
    "model.tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "\n",
    "model.generate(\"The capital of Germany is\", max_new_tokens=20, temperature=0)\n",
    "\n",
    "outputs, cache = model.run_with_cache(\"The capital of Australia is Canberra, the country’s largest inland city. It is located in the Australian Capital Territory (ACT), which is a federal territory. The city is located on the banks of the Molonglo River, 280 km south-west of Sydney.\\nThe city was founded in 1913 as a compromise between the six Australian colonies. The city was built to be the capital of the country, and it was named after the Aboriginal word for “meeting place”.\\nThe city is the seat of the Australian Parliament, and the Parliament House is the most important building in the city. The Parliament House is a modern building, which was built in the 1980s. It is a circular building, which is surrounded by a lake.\\nThe city is also the seat of the Australian Government. The city is the home of the High Court of Australia, the Supreme Court of the Australian Capital Territory, and the Australian Def\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einsum\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Goal: define a function to take in hyperparameters Alpha and K, model probe values, and model activations to calculate the new activations\n",
    "# Need to calculate alpha * sigma * theta\n",
    "# theta is the truthful direction given by probe parameter\n",
    "# sigma: standard deviation of head activation along truthful_dir (theta, either mass mean shift or probe weight direction)\n",
    "# adapted from get_interventions_dict in Kenneth Li github\n",
    "\n",
    "def get_act_std(head_activation, truthful_dir): # calculates standard deviations for one head\n",
    "    \"\"\"\n",
    "    head_activation: (batch, d_model,)\n",
    "    # truthful_dir: (d_model, )\n",
    "    \"\"\"\n",
    "    truthful_dir /= torch.norm(truthful_dir, dim=-1, keepdim=True)\n",
    "    proj_act = einops.einsum(head_activation, truthful_dir , \"b d_m, d_m -> b d_m\")\n",
    "    return torch.std(proj_act, dim=0) # (d_m)\n",
    "\n",
    "# truthful direction is difference in mean \n",
    "# returns (*, d_model)\n",
    "def get_mass_mean_dir(all_activations, truth_indices): # \n",
    "    \"\"\"\n",
    "    all_activations: (batch, *, d_model)\n",
    "    truth_indices: (batch, )\n",
    "    \"\"\"\n",
    "    # print(f\"shape of activations is {all_activations.shape}\")\n",
    "    # print(f\"shape of truth_indices is {truth_indices.shape}\")\n",
    "    true_mass_mean = torch.mean(all_activations[truth_indices == 1], dim=0) #(*, d_model)\n",
    "    false_mass_mean = torch.mean(all_activations[truth_indices == 0], dim=0)\n",
    "    # (* d_model)\n",
    "\n",
    "    return (true_mass_mean - false_mass_mean) / (true_mass_mean - false_mass_mean).norm()\n",
    "\n",
    "# truthful direction is probe weight\n",
    "# def get_probe_dirs(probe_list):\n",
    "#     # probe is a list (n_heads len) of LogisticRegression objects\n",
    "#     coefs = []\n",
    "#     for probe in probe_list:\n",
    "#         coefs.append(probe.coef_)\n",
    "        \n",
    "#     return torch.tensor(coefs, dtype=torch.float32, device=device)\n",
    "\n",
    "def get_probe_dir(probe):\n",
    "    probe_weights = torch.tensor(probe.coef_, dtype=torch.float32, device=device).squeeze()\n",
    "    return probe_weights / probe_weights.norm(dim=-1)\n",
    "\n",
    "\n",
    "# calculate the ITI addition (sigma * theta) for one head\n",
    "# uses either MMD or probe\n",
    "def calc_truth_proj(activation, use_MMD=False, use_probe=False, truth_indices=None, probe=None):\n",
    "    '''\n",
    "    activation is (batch, d_m)\n",
    "    '''\n",
    "    if use_MMD: # use mass mean direction -- average difference between true and false classified prompts (only one head)\n",
    "        assert truth_indices is not None\n",
    "        truthful_dir = get_mass_mean_dir(activation, truth_indices)\n",
    "    else: # probe -- just the coefficients of the probe\n",
    "        assert use_probe\n",
    "        assert probe is not None\n",
    "        truthful_dir = get_probe_dir(probe)\n",
    "\n",
    "    # print(f\"Old truthful dir direc is {truthful_dir.shape}\")\n",
    "    truthful_dir /= truthful_dir.norm(dim=-1)\n",
    "    # print(f\"New truthful dir direc is {truthful_dir.shape}\")\n",
    "    act_std = get_act_std(activation, truthful_dir)\n",
    "    \n",
    "    return einops.einsum(act_std, truthful_dir, \"d_m, d_m -> d_m\")\n",
    "\n",
    "def patch_activation_hook_fn(activations, hook: HookPoint, head, old_activations, use_MMD=True, use_probe=False, truth_indices=None, probe=None):\n",
    "    \"\"\"\n",
    "    activations: (batch, n_heads, d_model)\n",
    "    hook: HookPoint\n",
    "    term_to_add: (*, d_model)\n",
    "\n",
    "    A hook that is meant to act on the \"z\" (output) of a given head, and add the \"term_to_add\" on top of it. Only meant to work a certain head. Will broadcast.\n",
    "    \"\"\"\n",
    "    # print(f\"in hook fn, old act shape is {old_activations.shape}\")\n",
    "    term_to_add = calc_truth_proj(old_activations[:,head], use_MMD, use_probe, truth_indices, probe)\n",
    "    # print(f\"v shape is {term_to_add.shape}\")\n",
    "    # print(f\"activations shape is {activations.shape}\")\n",
    "    activations[:,-1,head] += term_to_add\n",
    "\n",
    "# Calculates new_activations for topk and adds temporary hooks\n",
    "def patch_top_activations(model, probe_accuracies, old_activations, topk=20, alpha=20, use_MMD=False, use_probe=False, truth_indices=None, probes=None):\n",
    "    '''\n",
    "    probe_accuracies: (n_layers, n_heads)\n",
    "    old_activations: (batch, n_layers, n_heads, d_model)\n",
    "\n",
    "    if use_probe is True, probes should be list in shape (n_layers, n_heads) filled with probes\n",
    "\n",
    "    Goes into every single activation, and then tells it to add the ITI\n",
    "    '''\n",
    "\n",
    "    # print(f\"old activations shape is {old_activations.shape}\")\n",
    "\n",
    "    top_head_indices = torch.topk(einops.rearrange(probe_accuracies, \"n_l n_h -> (n_l n_h)\"), k=topk).indices # take top k indices\n",
    "    top_head_bools = torch.zeros(size=(probe_accuracies.shape[0] * probe_accuracies.shape[1],)) # set all the ones that aren't top to 0\n",
    "\n",
    "    top_head_bools[top_head_indices] = torch.ones_like(top_head_bools[top_head_indices]) # set all the ones that are top to 1\n",
    "    top_head_bools = einops.rearrange(top_head_bools, \"(n_l n_h) -> n_l n_h\", n_l=model.cfg.n_layers) # rearrange back\n",
    "    \n",
    "    for layer in range(probe_accuracies.shape[0]):\n",
    "        for head in range(probe_accuracies.shape[1]):\n",
    "            if top_head_bools[layer, head] == 1:\n",
    "\n",
    "                if use_probe:\n",
    "                    patch_activation_with_head = partial(patch_activation_hook_fn, head = head, old_activations = old_activations[:, layer], use_MMD=False, use_probe=use_probe, truth_indices=None, probe=probes[layer][head])\n",
    "                else:\n",
    "                    patch_activation_with_head = partial(patch_activation_hook_fn, head = head, old_activations = old_activations[:, layer], use_MMD=use_MMD, use_probe=False, truth_indices=truth_indices, probe=None)\n",
    "                model.add_hook(utils.get_act_name(\"result\", layer), patch_activation_with_head)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
